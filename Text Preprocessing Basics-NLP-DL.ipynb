{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing Basics with NLTK**"
      ],
      "metadata": {
        "id": "JPBSSsTYbSpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROBLEM STATEMENT**\n",
        "Natural Language Processing (NLP) involves preparing raw text data for analysis and extracting meaningful insights. Effective text preprocessing is a crucial step in building robust NLP models. This project addresses the following problem:\n",
        "\n",
        "How can we preprocess textual data effectively to prepare it for downstream NLP\n",
        "tasks such as topic modeling, classification, or sentiment analysis?\n",
        "What tools and techniques can be used to clean, tokenize, and transform text for better machine understanding?\n",
        "This project demonstrates fundamental preprocessing techniques, including tokenization, stopword removal, and stemming, using the Natural Language Toolkit (NLTK). The processed text lays the groundwork for advanced NLP workflows."
      ],
      "metadata": {
        "id": "H0MzcUQya9ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Features**\n",
        "**Text Preprocessing:**\n",
        "1.Sentence and word tokenization.              \n",
        "2.Removal of stopwords to clean the text.         \n",
        "3.Stemming using NLTK's PorterStemmer to reduce words to their base forms.       \n",
        "     \n",
        "***Example Paragraph:*** A descriptive paragraph about Latent Semantic Analysis (LSA) is used as sample text for demonstrating these techniques. However, the project does not implement LSA itself but focuses on preparing text for NLP tasks."
      ],
      "metadata": {
        "id": "ZPqnC8-VbxOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbuXC3CQliFH",
        "outputId": "9a1b5ba0-be08-4324-ab2f-2d27d07e7f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.[1]\"\n",
        "paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Uoq8d5brl8WZ",
        "outputId": "7918bf5b-6898-47c9-b200-1b8535e3aba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.[1]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "bFKXIGfml8S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7jDh-pLl8Qi",
        "outputId": "c29fed60-1c51-4592-82eb-a214f29fee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkW8IaI9nPbj",
        "outputId": "e0bbfd84-2571-4645-c87b-db5d6d47dc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.',\n",
              " 'LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).',\n",
              " 'A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.',\n",
              " 'Documents are then compared by cosine similarity between any two columns.',\n",
              " 'Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.',\n",
              " '[1]']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer=PorterStemmer()\n",
        "stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdC-679gnkSi",
        "outputId": "8e9f44a6-5a0d-44e9-e46e-fdba6ebeb1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PorterStemmer>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem(\"HISTORICAL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MBijXpJpnuUo",
        "outputId": "d6f05b8f-52c8-4402-b5f9-3e8f91f9d113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'histor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "RG3x0ZhSnuRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA3m91M9oPlU",
        "outputId": "61b67a59-3a54-4c94-bca2-f3f78d5ba8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('HISTORICAL')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "llV8pKIxoFDG",
        "outputId": "cb999de6-10d2-42eb-c77a-2b42c35a2b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HISTORICAL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Oi2XARcQonmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =[]\n",
        "for i in range(len(sentences)):\n",
        "  review =re.sub(\"[^a-zA-Z]\",' ',sentences[i])\n",
        "  review=review.lower()\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "mNxYIQOoo8xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psEmqzjdqA53",
        "outputId": "1a7984f0-010c-4525-fdbd-10c9b95c1a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['latent semantic analysis  lsa  is a technique in natural language processing  in particular distributional semantics  of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms ',\n",
              " 'lsa assumes that words that are close in meaning will occur in similar pieces of text  the distributional hypothesis  ',\n",
              " 'a matrix containing word counts per document  rows represent unique words and columns represent each document  is constructed from a large piece of text and a mathematical technique called singular value decomposition  svd  is used to reduce the number of rows while preserving the similarity structure among columns ',\n",
              " 'documents are then compared by cosine similarity between any two columns ',\n",
              " 'values close to   represent very similar documents while values close to   represent very dissimilar documents ',\n",
              " '   ']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBZTRdKZqriK",
        "outputId": "7eff4967-1336-46f2-ef53-e0c7feca5c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=stopwords.words(\"english\")\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqbzm2oSqKxV",
        "outputId": "f173477c-d68a-4457-de66-299244c44884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in corpus:\n",
        "  words=nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuZUhHLGqKt6",
        "outputId": "089ee02b-1875-4e5c-baa7-11fe5140fcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latent\n",
            "semant\n",
            "analysi\n",
            "lsa\n",
            "techniqu\n",
            "natur\n",
            "languag\n",
            "process\n",
            "particular\n",
            "distribut\n",
            "semant\n",
            "analyz\n",
            "relationship\n",
            "set\n",
            "document\n",
            "term\n",
            "contain\n",
            "produc\n",
            "set\n",
            "concept\n",
            "relat\n",
            "document\n",
            "term\n",
            "lsa\n",
            "assum\n",
            "word\n",
            "close\n",
            "mean\n",
            "occur\n",
            "similar\n",
            "piec\n",
            "text\n",
            "distribut\n",
            "hypothesi\n",
            "matrix\n",
            "contain\n",
            "word\n",
            "count\n",
            "per\n",
            "document\n",
            "row\n",
            "repres\n",
            "uniqu\n",
            "word\n",
            "column\n",
            "repres\n",
            "document\n",
            "construct\n",
            "larg\n",
            "piec\n",
            "text\n",
            "mathemat\n",
            "techniqu\n",
            "call\n",
            "singular\n",
            "valu\n",
            "decomposit\n",
            "svd\n",
            "use\n",
            "reduc\n",
            "number\n",
            "row\n",
            "preserv\n",
            "similar\n",
            "structur\n",
            "among\n",
            "column\n",
            "document\n",
            "compar\n",
            "cosin\n",
            "similar\n",
            "two\n",
            "column\n",
            "valu\n",
            "close\n",
            "repres\n",
            "similar\n",
            "document\n",
            "valu\n",
            "close\n",
            "repres\n",
            "dissimilar\n",
            "document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(binary=True,ngram_range=(2,2))"
      ],
      "metadata": {
        "id": "rkBJ-Bm0sddt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "Ui-NAwMxtUMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjXEVi4Ztikx",
        "outputId": "450c5492-1a07-43e5-cad9-4b0975424e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latent semantic': 46,\n",
              " 'semantic analysis': 75,\n",
              " 'analysis lsa': 1,\n",
              " 'lsa is': 48,\n",
              " 'is technique': 42,\n",
              " 'technique in': 86,\n",
              " 'in natural': 38,\n",
              " 'natural language': 52,\n",
              " 'language processing': 44,\n",
              " 'processing in': 65,\n",
              " 'in particular': 39,\n",
              " 'particular distributional': 60,\n",
              " 'distributional semantics': 29,\n",
              " 'semantics of': 76,\n",
              " 'of analyzing': 55,\n",
              " 'analyzing relationships': 2,\n",
              " 'relationships between': 69,\n",
              " 'between set': 12,\n",
              " 'set of': 77,\n",
              " 'of documents': 57,\n",
              " 'documents and': 32,\n",
              " 'and the': 6,\n",
              " 'the terms': 96,\n",
              " 'terms they': 87,\n",
              " 'they contain': 98,\n",
              " 'contain by': 22,\n",
              " 'by producing': 14,\n",
              " 'producing set': 66,\n",
              " 'of concepts': 56,\n",
              " 'concepts related': 20,\n",
              " 'related to': 68,\n",
              " 'to the': 101,\n",
              " 'the documents': 93,\n",
              " 'and terms': 5,\n",
              " 'lsa assumes': 47,\n",
              " 'assumes that': 10,\n",
              " 'that words': 91,\n",
              " 'words that': 114,\n",
              " 'that are': 90,\n",
              " 'are close': 8,\n",
              " 'close in': 16,\n",
              " 'in meaning': 37,\n",
              " 'meaning will': 51,\n",
              " 'will occur': 111,\n",
              " 'occur in': 54,\n",
              " 'in similar': 40,\n",
              " 'similar pieces': 79,\n",
              " 'pieces of': 63,\n",
              " 'of text': 59,\n",
              " 'text the': 89,\n",
              " 'the distributional': 92,\n",
              " 'distributional hypothesis': 28,\n",
              " 'matrix containing': 50,\n",
              " 'containing word': 23,\n",
              " 'word counts': 112,\n",
              " 'counts per': 25,\n",
              " 'per document': 61,\n",
              " 'document rows': 31,\n",
              " 'rows represent': 73,\n",
              " 'represent unique': 71,\n",
              " 'unique words': 103,\n",
              " 'words and': 113,\n",
              " 'and columns': 3,\n",
              " 'columns represent': 18,\n",
              " 'represent each': 70,\n",
              " 'each document': 35,\n",
              " 'document is': 30,\n",
              " 'is constructed': 41,\n",
              " 'constructed from': 21,\n",
              " 'from large': 36,\n",
              " 'large piece': 45,\n",
              " 'piece of': 62,\n",
              " 'text and': 88,\n",
              " 'and mathematical': 4,\n",
              " 'mathematical technique': 49,\n",
              " 'technique called': 85,\n",
              " 'called singular': 15,\n",
              " 'singular value': 82,\n",
              " 'value decomposition': 105,\n",
              " 'decomposition svd': 26,\n",
              " 'svd is': 84,\n",
              " 'is used': 43,\n",
              " 'used to': 104,\n",
              " 'to reduce': 99,\n",
              " 'reduce the': 67,\n",
              " 'the number': 94,\n",
              " 'number of': 53,\n",
              " 'of rows': 58,\n",
              " 'rows while': 74,\n",
              " 'while preserving': 109,\n",
              " 'preserving the': 64,\n",
              " 'the similarity': 95,\n",
              " 'similarity structure': 81,\n",
              " 'structure among': 83,\n",
              " 'among columns': 0,\n",
              " 'documents are': 33,\n",
              " 'are then': 9,\n",
              " 'then compared': 97,\n",
              " 'compared by': 19,\n",
              " 'by cosine': 13,\n",
              " 'cosine similarity': 24,\n",
              " 'similarity between': 80,\n",
              " 'between any': 11,\n",
              " 'any two': 7,\n",
              " 'two columns': 102,\n",
              " 'values close': 106,\n",
              " 'close to': 17,\n",
              " 'to represent': 100,\n",
              " 'represent very': 72,\n",
              " 'very similar': 108,\n",
              " 'similar documents': 78,\n",
              " 'documents while': 34,\n",
              " 'while values': 110,\n",
              " 'very dissimilar': 107,\n",
              " 'dissimilar documents': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OdDujyPutvsi",
        "outputId": "a9d43be8-efbc-45e4-d5f2-8608194b9ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lsa assumes that words that are close in meaning will occur in similar pieces of text  the distributional hypothesis  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8tJI9nst1H4",
        "outputId": "ec026c58-41ec-464a-f4b7-d69a56fbc870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv=TfidfVectorizer(ngram_range=(1,1))\n",
        "x=cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "8A_-yelguJkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B9HmPQBxIQI",
        "outputId": "6c2fc624-935c-4d90-adec-1d3579a17468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latent': 31,\n",
              " 'semantic': 52,\n",
              " 'analysis': 1,\n",
              " 'lsa': 32,\n",
              " 'is': 28,\n",
              " 'technique': 60,\n",
              " 'in': 27,\n",
              " 'natural': 36,\n",
              " 'language': 29,\n",
              " 'processing': 45,\n",
              " 'particular': 40,\n",
              " 'distributional': 21,\n",
              " 'semantics': 53,\n",
              " 'of': 39,\n",
              " 'analyzing': 2,\n",
              " 'relationships': 49,\n",
              " 'between': 7,\n",
              " 'set': 54,\n",
              " 'documents': 23,\n",
              " 'and': 3,\n",
              " 'the': 64,\n",
              " 'terms': 61,\n",
              " 'they': 66,\n",
              " 'contain': 15,\n",
              " 'by': 8,\n",
              " 'producing': 46,\n",
              " 'concepts': 13,\n",
              " 'related': 48,\n",
              " 'to': 67,\n",
              " 'assumes': 6,\n",
              " 'that': 63,\n",
              " 'words': 77,\n",
              " 'are': 5,\n",
              " 'close': 10,\n",
              " 'meaning': 35,\n",
              " 'will': 75,\n",
              " 'occur': 38,\n",
              " 'similar': 55,\n",
              " 'pieces': 43,\n",
              " 'text': 62,\n",
              " 'hypothesis': 26,\n",
              " 'matrix': 34,\n",
              " 'containing': 16,\n",
              " 'word': 76,\n",
              " 'counts': 18,\n",
              " 'per': 41,\n",
              " 'document': 22,\n",
              " 'rows': 51,\n",
              " 'represent': 50,\n",
              " 'unique': 69,\n",
              " 'columns': 11,\n",
              " 'each': 24,\n",
              " 'constructed': 14,\n",
              " 'from': 25,\n",
              " 'large': 30,\n",
              " 'piece': 42,\n",
              " 'mathematical': 33,\n",
              " 'called': 9,\n",
              " 'singular': 57,\n",
              " 'value': 71,\n",
              " 'decomposition': 19,\n",
              " 'svd': 59,\n",
              " 'used': 70,\n",
              " 'reduce': 47,\n",
              " 'number': 37,\n",
              " 'while': 74,\n",
              " 'preserving': 44,\n",
              " 'similarity': 56,\n",
              " 'structure': 58,\n",
              " 'among': 0,\n",
              " 'then': 65,\n",
              " 'compared': 12,\n",
              " 'cosine': 17,\n",
              " 'any': 4,\n",
              " 'two': 68,\n",
              " 'values': 72,\n",
              " 'very': 73,\n",
              " 'dissimilar': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gP89eBAlw-lu",
        "outputId": "53f95f20-c1ba-48b9-eae3-0a8c184d692a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lsa assumes that words that are close in meaning will occur in similar pieces of text  the distributional hypothesis  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Dz_3EixCLb",
        "outputId": "bed44566-9931-415a-a773-f6ac5c95f8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.19139971, 0.2334102 , 0.        , 0.        , 0.        ,\n",
              "        0.19139971, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.19139971, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.2334102 , 0.38279941, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19139971, 0.        , 0.        ,\n",
              "        0.2334102 , 0.        , 0.        , 0.2334102 , 0.16159278,\n",
              "        0.        , 0.        , 0.        , 0.2334102 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.19139971, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19139971, 0.46682041, 0.16159278,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.2334102 , 0.        , 0.19139971]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}